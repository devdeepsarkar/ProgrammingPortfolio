{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42230,"status":"ok","timestamp":1741784925422,"user":{"displayName":"MultiDL","userId":"05750408194492004447"},"user_tz":-330},"id":"aBK9fuXrOI-F","outputId":"ee162340-70e1-4e49-8485-8c16ea0292c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sekk8E3IP2Kl"},"outputs":[],"source":["# import torch\n","# print(torch.cuda.is_available())  # Should print True\n","# print(torch.cuda.get_device_name(0))  # Should print 'T4'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Iz1v-YTQTk3"},"outputs":[],"source":["# Check if GPU is available\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# print(f\"Using device: {device}\")"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"x68o5cHiP7mn","executionInfo":{"status":"ok","timestamp":1741784931815,"user_tz":-330,"elapsed":4162,"user":{"displayName":"MultiDL","userId":"05750408194492004447"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import re\n","from collections import defaultdict, Counter\n","import math"]},{"cell_type":"code","source":["device = torch.device(\"cuda\")"],"metadata":{"id":"4Hqu7sXtjX0P","executionInfo":{"status":"ok","timestamp":1741775714678,"user_tz":-330,"elapsed":6,"user":{"displayName":"MultiDL","userId":"05750408194492004447"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"NyjqBzHvOiK-","executionInfo":{"status":"ok","timestamp":1741784936370,"user_tz":-330,"elapsed":2259,"user":{"displayName":"MultiDL","userId":"05750408194492004447"}}},"outputs":[],"source":["data1 = pd.read_csv('/content/drive/MyDrive/CSV_New_Vector/Data_CSV_100_500/50MB.csv')\n","# data2 = pd.read_csv('/content/drive/MyDrive/CSV_New_Vector/data2.csv', header=None)\n","# data3 = pd.read_csv('/content/drive/MyDrive/CSV_New_Vector/data3.csv')\n","# data4 = pd.read_csv('/content/drive/MyDrive/CSV_New_Vector/data4.csv')\n","# class_0 = pd.read_csv('/content/drive/MyDrive/csv/accident.csv', index_col=[0])\n","# texts = class_0['news_articles'].dropna().tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N3V9DeG9fxTQ"},"outputs":[],"source":["# data1 = pd.read_csv(\"your_file.csv\", header=None)\n","# data2.columns = [\"news_articles\"]  # Set column names manually"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7dItiYw9ePpd"},"outputs":[],"source":["# print(data1.columns)\n","# print(data1.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SDkCmBBxdQq6"},"outputs":[],"source":["# news = pd.concat([data1, data2, data3, data4])"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":397,"status":"ok","timestamp":1741784937089,"user":{"displayName":"MultiDL","userId":"05750408194492004447"},"user_tz":-330},"id":"lP0bwS3meodP","outputId":"43be8b5f-2308-443a-9270-8e3c38e52fef"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                       news_articles\n","0  भारतीय अंतरिक्ष अनुसंधान संगठन (इसरो) ने चंद्र...\n","1  ब्लू होल समुद्र के नीचे बड़ी गुफाएं/गड्ढे या स...\n","2  नासा सन ऐंड स्पेस के आधिकारिक X हैंडल ने सूर्य...\n","3  नासा द्वारा दुनिया की करीब 30 लाख नदियों पर कि...\n","4  वैज्ञानिकों ने एक निएंडरथल महिला का चेहरा री-क..."],"text/html":["\n","  <div id=\"df-c996f6e6-ff2a-4a8b-a1ef-9fef75bd3e30\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>news_articles</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>भारतीय अंतरिक्ष अनुसंधान संगठन (इसरो) ने चंद्र...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ब्लू होल समुद्र के नीचे बड़ी गुफाएं/गड्ढे या स...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>नासा सन ऐंड स्पेस के आधिकारिक X हैंडल ने सूर्य...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>नासा द्वारा दुनिया की करीब 30 लाख नदियों पर कि...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>वैज्ञानिकों ने एक निएंडरथल महिला का चेहरा री-क...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c996f6e6-ff2a-4a8b-a1ef-9fef75bd3e30')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-c996f6e6-ff2a-4a8b-a1ef-9fef75bd3e30 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-c996f6e6-ff2a-4a8b-a1ef-9fef75bd3e30');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-8fad0952-fe24-489b-a7e4-265e2769ce6d\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8fad0952-fe24-489b-a7e4-265e2769ce6d')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-8fad0952-fe24-489b-a7e4-265e2769ce6d button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"data1","summary":"{\n  \"name\": \"data1\",\n  \"rows\": 56948,\n  \"fields\": [\n    {\n      \"column\": \"news_articles\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 21478,\n        \"samples\": [\n          \"\\u092c\\u0940\\u091c\\u0947\\u092a\\u0940 \\u0928\\u0947\\u0924\\u093e \\u0905\\u092e\\u093f\\u0924 \\u092e\\u093e\\u0932\\u0935\\u0940\\u092f \\u0928\\u0947 'X' \\u092a\\u0930 \\u090f\\u0915 \\u092e\\u0939\\u093f\\u0932\\u093e \\u0935 \\u092a\\u0941\\u0930\\u0941\\u0937 \\u0915\\u094b \\u0921\\u0902\\u0921\\u0947 \\u0938\\u0947 \\u092a\\u0940\\u091f\\u0924\\u0947 \\u090f\\u0915 \\u0936\\u0916\\u094d\\u0938 \\u0915\\u093e \\u0935\\u0940\\u0921\\u093f\\u092f\\u094b \\u0936\\u0947\\u092f\\u0930 \\u0915\\u0930\\u0924\\u0947 \\u0939\\u0941\\u090f \\u0932\\u093f\\u0916\\u093e, \\\"\\u092d\\u093e\\u0930\\u0924 \\u0915\\u094b \\u091f\\u0940\\u090f\\u092e\\u0938\\u0940 \\u0936\\u093e\\u0938\\u093f\\u0924 \\u092a\\u0936\\u094d\\u091a\\u093f\\u092e \\u092c\\u0902\\u0917\\u093e\\u0932 \\u092e\\u0947\\u0902 \\u0936\\u0930\\u093f\\u092f\\u093e \\u0905\\u0926\\u093e\\u0932\\u0924\\u094b\\u0902 \\u0915\\u0940 \\u0935\\u093e\\u0938\\u094d\\u0924\\u0935\\u093f\\u0915\\u0924\\u093e \\u0938\\u0947 \\u0905\\u0935\\u0917\\u0924 \\u0939\\u094b\\u0928\\u093e \\u091a\\u093e\\u0939\\u093f\\u090f\\u0964\\\" \\u0909\\u0928\\u094d\\u0939\\u094b\\u0902\\u0928\\u0947 \\u0932\\u093f\\u0916\\u093e, \\\"\\u092e\\u0941\\u0916\\u094d\\u092f\\u092e\\u0902\\u0924\\u094d\\u0930\\u0940 \\u092e\\u092e\\u0924\\u093e \\u092c\\u0928\\u0930\\u094d\\u091c\\u0940 \\u092e\\u0939\\u093f\\u0932\\u093e\\u0913\\u0902 \\u0915\\u0947 \\u0932\\u093f\\u090f \\u0905\\u092d\\u093f\\u0936\\u093e\\u092a \\u0939\\u0948\\u0902\\u0964\\\" \\u092e\\u093e\\u0932\\u0935\\u0940\\u092f \\u0928\\u0947 \\u0926\\u093e\\u0935\\u093e \\u0915\\u093f\\u092f\\u093e \\u0939\\u0948 \\u0915\\u093f \\u092f\\u0939 \\u0935\\u0940\\u0921\\u093f\\u092f\\u094b \\u0909\\u0924\\u094d\\u0924\\u0930\\u0940 \\u0926\\u093f\\u0928\\u093e\\u091c\\u092a\\u0941\\u0930 \\u0915\\u093e \\u0939\\u0948\\u0964\",\n          \"\\u0909\\u0921\\u093c\\u093e\\u0928 \\u0915\\u0947 \\u0926\\u094c\\u0930\\u093e\\u0928 \\u0935\\u093f\\u092e\\u093e\\u0928 \\u0915\\u0940 \\u0938\\u0940\\u091f\\u094b\\u0902 \\u092a\\u0930 \\u090f\\u0915-\\u0926\\u0942\\u0938\\u0930\\u0947 \\u0938\\u0947 \\u0932\\u093f\\u092a\\u091f\\u0915\\u0930 \\u0932\\u0947\\u091f\\u0947 \\u0926\\u093f\\u0916 \\u0930\\u0939\\u0947 \\u090f\\u0915 \\u0915\\u092a\\u0932 \\u0915\\u0940 \\u0924\\u0938\\u094d\\u0935\\u0940\\u0930\\u0947\\u0902 \\u0911\\u0928\\u0932\\u093e\\u0907\\u0928 \\u0935\\u093e\\u092f\\u0930\\u0932 \\u0939\\u094b \\u0917\\u0908 \\u0939\\u0948\\u0902\\u0964 \\u090f\\u0915 \\u0905\\u092e\\u0947\\u0930\\u093f\\u0915\\u0940 \\u0936\\u0916\\u094d\\u0938 \\u0926\\u094d\\u0935\\u093e\\u0930\\u093e \\u0907\\u0938\\u0915\\u0940 \\u0924\\u0938\\u094d\\u0935\\u0940\\u0930\\u0947\\u0902 \\u091f\\u094d\\u0935\\u0940\\u091f \\u0915\\u093f\\u090f \\u091c\\u093e\\u0928\\u0947 \\u0915\\u0947 \\u092c\\u093e\\u0926 \\u090f\\u0915 X \\u092f\\u0942\\u091c\\u093c\\u0930 \\u0928\\u0947 \\u0932\\u093f\\u0916\\u093e, \\\"\\u092b\\u094d\\u0932\\u093e\\u0907\\u091f \\u0905\\u091f\\u0947\\u0902\\u0921\\u0947\\u0902\\u091f \\u0928\\u0947 \\u0915\\u0948\\u0938\\u0947 \\u0915\\u0941\\u091b \\u0928\\u0939\\u0940\\u0902 \\u0915\\u0939\\u093e?\\\" \\u0935\\u0939\\u0940\\u0902 \\u090f\\u0915 \\u0905\\u0928\\u094d\\u092f \\u092f\\u0942\\u091c\\u093c\\u0930 \\u0928\\u0947 \\u0915\\u0939\\u093e, \\\"\\u0907\\u0938 \\u0924\\u0930\\u0939 \\u0915\\u093e \\u092a\\u094d\\u092f\\u093e\\u0930 \\u092e\\u0941\\u091d\\u0947 \\u092a\\u0930\\u0947\\u0936\\u093e\\u0928 \\u0915\\u0930\\u0924\\u093e \\u0939\\u0948\\u0964\\\" \",\n          \"\\u0936\\u0941\\u0915\\u094d\\u0930\\u0935\\u093e\\u0930 \\u0936\\u093e\\u092e \\u0915\\u094b \\u092a\\u0947\\u0930\\u093f\\u0938 2024 \\u0913\\u0932\\u0902\\u092a\\u093f\\u0915\\u094d\\u0938 \\u0915\\u0947 \\u0909\\u0926\\u094d\\u0918\\u093e\\u091f\\u0928 \\u0938\\u092e\\u093e\\u0930\\u094b\\u0939 \\u0915\\u0947 \\u0926\\u094c\\u0930\\u093e\\u0928 \\u091b\\u0924\\u094b\\u0902 \\u092a\\u0930 \\u0926\\u094c\\u0921\\u093c\\u0928\\u0947 \\u0935\\u093e\\u0932\\u0947 \\u0939\\u0941\\u0921 \\u0935\\u093e\\u0932\\u0940 \\u092a\\u094b\\u0936\\u093e\\u0915 \\u092a\\u0939\\u0928\\u0947 \\u0928\\u0915\\u093e\\u092c\\u092a\\u094b\\u0936 \\u092e\\u0936\\u093e\\u0932 \\u0935\\u093e\\u0939\\u0915 \\u0915\\u0940 \\u092a\\u0939\\u091a\\u093e\\u0928 \\u0905\\u092c \\u0924\\u0915 \\u0905\\u091c\\u094d\\u091e\\u093e\\u0924 \\u0939\\u0948\\u0964 \\u0915\\u0908 \\u0938\\u094b\\u0936\\u0932 \\u092e\\u0940\\u0921\\u093f\\u092f\\u093e \\u092f\\u0942\\u095b\\u0930\\u094d\\u0938 \\u0928\\u0947 \\u092e\\u0936\\u093e\\u0932 \\u0935\\u093e\\u0939\\u0915 \\u0915\\u0940 \\u092a\\u0939\\u091a\\u093e\\u0928 \\u0915\\u0947 \\u092c\\u093e\\u0930\\u0947 \\u092e\\u0947\\u0902 \\u0905\\u0928\\u0941\\u092e\\u093e\\u0928 \\u0932\\u0917\\u093e\\u092f\\u093e \\u0939\\u0948\\u0964 \\u090f\\u0915 \\u0928\\u0947 \\u092e\\u091c\\u093c\\u093e\\u0915 \\u092e\\u0947\\u0902 \\u0932\\u093f\\u0916\\u093e, \\\"\\u0928\\u0915\\u093e\\u092c\\u092a\\u094b\\u0936 \\u092e\\u0936\\u093e\\u0932 \\u0935\\u093e\\u0939\\u0915 \\u0939\\u092e\\u093e\\u0930\\u093e \\u0938\\u094d\\u092a\\u093e\\u0907\\u0921\\u0930\\u0935\\u0930\\u094d\\u0938 \\u0938\\u094d\\u092a\\u093e\\u0907\\u0921\\u0930\\u092e\\u0948\\u0928 \\u0939\\u0948... \\u0939\\u0948 \\u0928?\\\"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":4}],"source":["data1.head()"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"POSW4Ca-O4as","executionInfo":{"status":"ok","timestamp":1741784939191,"user_tz":-330,"elapsed":9,"user":{"displayName":"MultiDL","userId":"05750408194492004447"}}},"outputs":[],"source":["texts = data1['news_articles'].dropna().tolist()"]},{"cell_type":"markdown","metadata":{"id":"3su7EmW5WIZH"},"source":["# **Preprocessing and Tokenization**"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"7vkhyjzIQyIK","executionInfo":{"status":"ok","timestamp":1741784941754,"user_tz":-330,"elapsed":22,"user":{"displayName":"MultiDL","userId":"05750408194492004447"}}},"outputs":[],"source":["# Custom stop words and tokenizer\n","stop_words = set(['।','',' ',' ','!','! ','!  ','! !','! ! ','! ! !','?','अरे', 'चैनल्स', 'ये','ही','तुमसे','बार','आप','तुम्हारे','तु','रहा','कुछ','कभी',\n","              'एक','तुम','होता','नहीं','कितनी','पर','तू','हो','है','क्यों','एप','कर','काम','रहे','बातें','लग','आता','ये चैनल्स','करनी','अपना','पैक्स','चीज़','क्या','अरे ये','करा','मैं',\n","              'अत','अपना','अपनी','अपने','अभी','अंदर','आदि','आप','इत्यादि','इन','इनका','इन्हीं','इन्हें','इन्हों','इस','इसका','इसकी','इसके','इसमें',\n","              'इसी','इसे','उन','उनका','उनकी','उनके','उनको','उन्हीं','उन्हें','उन्हों','उस','उसके','उसी','उसे','एक','एवं','एस','ऐसे','और','कई',\n","              'कर','करता','करते','करना','करने','करें','कहते','कहा','का','काफ़ी','कि','कितना','किन्हें','किन्हों','किया','किर','किस','किसी','किसे','की',\n","              'कुछ','कुल','के','को','कोई','कौन','कौन','बही','बहुत','बाद','बाला','बिलकुल','भी','भीतर','मगर','मानो','मे','में','यदि','यह','यहाँ','यही',\n","              'या','यिह','ये','रखें','रहा','रहे','ऱ्वासा','लिए','लिये','लेकिन','व','वग़ैरह','वर्ग','वह','वहाँ','वहीं','वाले','वुह','वे','सकता','सकते','सबसे',\n","              'सभी','साथ','साबुत','साभ','सारा','से','सो','संग','ही','हुआ','हुई','हुए','है','हैं','हो','होता','होती','होते','होना','होने'])\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"8VynVPB1QFXJ","executionInfo":{"status":"ok","timestamp":1741784947961,"user_tz":-330,"elapsed":3883,"user":{"displayName":"MultiDL","userId":"05750408194492004447"}}},"outputs":[],"source":["def preprocess(text):\n","    # Convert to lowercase\n","    # text = text.lower()\n","\n","    # # Remove single quote, double quote, full stop\n","    text = re.sub(r\"[.'\\\"]\", \"\", text)\n","    # Remove English characters, special characters, and new lines\n","    text = re.sub(r'[^\\u0900-\\u097F\\s]', '', text)  # Keeping only Hindi characters and spaces\n","    #remove numbers\n","    text= re.sub(r\"\\d+\", \"\", text)\n","    text = re.sub(r'http[s]?://\\S+', '', text)  # Remove hyperlinks\n","    text = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F]', '', text)  # Remove non-printable characters\n","    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces and new lines\n","    # Tokenize (split into words)\n","    words = text.split()\n","\n","    # Remove stopwords\n","    filtered_words = [word for word in words if word not in stop_words]\n","\n","    return filtered_words  # Return as a list of tokens\n","\n","# Process the entire dataset\n","tokenized_sentences = [preprocess(text) for text in texts]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UEDFq9eFQedE"},"outputs":[],"source":["# print(tokenized_sentences)"]},{"cell_type":"markdown","metadata":{"id":"0Qg2Lu0zWWFV"},"source":["# **Vocabulary Mapping**"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"4drsfGjCS95s","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741784948414,"user_tz":-330,"elapsed":452,"user":{"displayName":"MultiDL","userId":"05750408194492004447"}},"outputId":"078ce096-91bd-4ad3-9ec8-709084838b64"},"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary Size: 56123\n"]}],"source":["word_counts = Counter(word for sentence in tokenized_sentences for word in sentence)\n","vocab = list(word_counts.keys())\n","vocab_size = len(vocab)\n","word_to_id = {word: i for i, word in enumerate(vocab)}\n","id_to_word = {i: word for word, i in word_to_id.items()}\n","\n","print(f\"Vocabulary Size: {vocab_size}\")"]},{"cell_type":"markdown","source":["***Removing min occurence words***"],"metadata":{"id":"VEBrppuIEPaR"}},{"cell_type":"code","execution_count":33,"metadata":{"id":"jfbWk9t3szc8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741776782773,"user_tz":-330,"elapsed":1108,"user":{"displayName":"MultiDL","userId":"05750408194492004447"}},"outputId":"58377fd8-1b51-4fa0-c5df-cd42bf2fd32d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Filtered/Actual Vocabulary Size: 29300\n","Extra Stop Words Saved (26823 words) in extra_stop_words.txt\n"]}],"source":["# # Set minimum occurrence threshold\n","# min_occurrence = 3\n","\n","# # Filter vocabulary (Keep high-frequency words)\n","# vocab = {word: idx for word, idx in word_to_id.items() if word_counts[word] > min_occurrence}\n","\n","# # Identify extra stop words (Low-frequency words)\n","# extra_stop_words = [word for word in word_to_id if word_counts[word] <= min_occurrence]\n","\n","# # Save extra stop words to a text file\n","# with open(\"/content/drive/MyDrive/DummyFolder/extra_stop_words_100MB.txt\", \"w\", encoding=\"utf-8\") as f:\n","#     for word in extra_stop_words:\n","#         f.write(word + \"\\n\")\n","\n","# print(f\"Filtered/Actual Vocabulary Size: {len(vocab)}\")\n","# print(f\"Extra Stop Words Saved ({len(extra_stop_words)} words) in extra_stop_words.txt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zw32h9IP91mS"},"outputs":[],"source":["# # Load extra stop words from the text file and convert them into a set\n","# with open(\"/content/drive/MyDrive/DummyFolder/extra_stop_words_100MB.txt\", \"r\", encoding=\"utf-8\") as f:\n","#     extra_stop_words = set(f.read().splitlines())\n","\n","# # Verify the set\n","# print(f\"Loaded {len(extra_stop_words)} extra stop words.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6PAO6CZh-BgK"},"outputs":[],"source":["# # Combine both sets\n","# combined_stop_words = stop_words.union(extra_stop_words)\n","# print(f\"Total stop words: {len(combined_stop_words)}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a_Rg67JY_Aob"},"outputs":[],"source":["# def remove_extra_stopwords(text):\n","#     # Convert to lowercase\n","#     # text = text.lower()\n","\n","#     # # Remove single quote, double quote, full stop\n","#     text = re.sub(r\"[.'\\\"]\", \"\", text)\n","#     # Remove English characters, special characters, and new lines\n","#     text = re.sub(r'[^\\u0900-\\u097F\\s]', '', text)  # Keeping only Hindi characters and spaces\n","#     #remove numbers\n","#     text= re.sub(r\"\\d+\", \"\", text)\n","#     text = re.sub(r'http[s]?://\\S+', '', text)  # Remove hyperlinks\n","#     text = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F]', '', text)  # Remove non-printable characters\n","#     text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces and new lines\n","#     # Tokenize (split into words)\n","#     words = text.split()\n","\n","#     # Remove stopwords\n","#     filtered_words = [word for word in words if word not in combined_stop_words]\n","\n","#     return filtered_words  # Return as a list of tokens\n","\n","# # Example: Process the entire dataset\n","# tokenized_actual_sentences = [remove_extra_stopwords(text) for text in texts]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hJg9CMyD_qAj"},"outputs":[],"source":["# actual_word_counts = Counter(word for sentence in tokenized_actual_sentences for word in sentence)\n","# actual_vocab = list(actual_word_counts.keys())\n","# actual_vocab_size = len(actual_vocab)\n","# actual_word_to_id = {word: i for i, word in enumerate(actual_vocab)}\n","# actual_id_to_word = {i: word for word, i in actual_word_to_id.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"38guFC24u-ea"},"outputs":[],"source":["# word_to_id = {word: i for i, word in enumerate(vocab)}\n","# id_to_word = {i: word for word, i in word_to_id.items()}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WTo3tjfRWNef"},"outputs":[],"source":["# print(f\"Vocabulary Size: {actual_vocab_size}\")"]},{"cell_type":"code","source":["total_length_import = sum(len(text.split()) for text in texts)\n","print(f\"Total Length at Import: {total_length_import}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F4OKMfyDjg1_","executionInfo":{"status":"ok","timestamp":1741784955508,"user_tz":-330,"elapsed":317,"user":{"displayName":"MultiDL","userId":"05750408194492004447"}},"outputId":"a7bec81b-4e08-4243-f2d2-a2ad2d0785af"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Total Length at Import: 3939982\n"]}]},{"cell_type":"code","source":["total_length_before = sum(len(sentence) for sentence in tokenized_sentences)\n","print(f\"Total Length After Pre-Processing: {total_length_before}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ckIxOTJmiSME","executionInfo":{"status":"ok","timestamp":1741784956684,"user_tz":-330,"elapsed":27,"user":{"displayName":"MultiDL","userId":"05750408194492004447"}},"outputId":"64994ca8-17b4-40b8-c32d-8854862cd677"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Total Length After Pre-Processing: 2535219\n"]}]},{"cell_type":"code","source":["# total_length_after = sum(len(sentence) for sentence in tokenized_actual_sentences)\n","# print(f\"Total Length After Filtering: {total_length_after}\")\n"],"metadata":{"id":"ITjVuPT6iVos"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reduction_import_to_tokenized = ((total_length_import - total_length_before) / total_length_import) * 100\n","# reduction_tokenized_to_filtered = ((total_length_before - total_length_after) / total_length_before) * 100\n","# reduction_import_to_filtered = ((total_length_import - total_length_after) / total_length_import) * 100\n","\n","print(f\"Reduction from Import after Pre-Processing: {reduction_import_to_tokenized:.2f}%\")\n","# print(f\"Reduction from Tokenized to Pre-processing: {reduction_tokenized_to_filtered:.2f}%\")\n","# print(f\"Total Reduction from Import to Filtered: {reduction_import_to_filtered:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dsvh3sdLjMC4","executionInfo":{"status":"ok","timestamp":1741784959275,"user_tz":-330,"elapsed":8,"user":{"displayName":"MultiDL","userId":"05750408194492004447"}},"outputId":"94bc7e15-6a50-4025-aa0b-3f4b59021b3c"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Reduction from Import after Pre-Processing: 35.65%\n"]}]},{"cell_type":"markdown","metadata":{"id":"ZWFvw0XcWgQk"},"source":["# **Cooccurrence Matrix using Distance Based Weighting**"]},{"cell_type":"code","source":["# # Convert sentences to ID arrays\n","# import torch\n","# import numpy as np\n","# import scipy.sparse\n","# token_ids = [[word_to_id[word] for word in sent] for sent in tokenized_sentences]\n","\n","# # Convert to PyTorch GPU tensors\n","# token_ids = [torch.tensor(sent, dtype=torch.int32, device=device) for sent in token_ids]\n","\n","# # Vocabulary size\n","# vocab_size = len(word_to_id)\n","\n","# # Window size\n","# window_size = 8\n","\n","# # File path for storage\n","# save_path = \"/content/drive/MyDrive/DummyFolder/cooccurrence_matrix_50MBNEW.npz\"\n","\n","# # Initialize storage variables\n","# word_ids_list, context_ids_list, values_list = [], [], []\n","# batch_size = 10000  # Adjust based on available memory\n","\n","# def compute_cooccurrence(token_ids):\n","#     \"\"\"Compute co-occurrence matrix using a sliding window approach.\"\"\"\n","#     global word_ids_list, context_ids_list, values_list\n","\n","#     for idx, sentence in enumerate(token_ids):\n","#         sent_len = sentence.shape[0]\n","#         for i in range(sent_len):\n","#             word_id = sentence[i].item()\n","#             start = max(0, i - window_size)\n","#             end = min(sent_len, i + window_size + 1)\n","\n","#             for j in range(start, end):\n","#                 if i != j:\n","#                     context_id = sentence[j].item()\n","#                     distance = abs(j - i)\n","#                     increment = 1.0 / distance\n","#                     word_ids_list.append(word_id)\n","#                     context_ids_list.append(context_id)\n","#                     values_list.append(increment)\n","\n","#             # Save data when reaching batch size\n","#             if len(word_ids_list) >= batch_size:\n","#                 save_to_disk()\n","\n","#         # Periodic flushing to avoid memory overflow\n","#         if idx % 1000 == 0:\n","#             save_to_disk()\n","\n","#     # Final save\n","#     save_to_disk()\n","\n","# import os\n","\n","# def save_to_disk():\n","#     \"\"\"Append co-occurrence data to an existing sparse matrix instead of overwriting.\"\"\"\n","#     global word_ids_list, context_ids_list, values_list\n","\n","#     if word_ids_list:\n","#         # Convert current batch to sparse matrix\n","#         new_matrix = scipy.sparse.coo_matrix(\n","#             (values_list, (word_ids_list, context_ids_list)),\n","#             shape=(vocab_size, vocab_size)\n","#         )\n","\n","#         if os.path.exists(save_path):\n","#             # Load existing matrix\n","#             existing_matrix = scipy.sparse.load_npz(save_path)\n","\n","#             # Sum the new co-occurrence values with the existing ones\n","#             new_matrix = existing_matrix + new_matrix\n","\n","#         # Save updated matrix\n","#         # print('Saved 10000 batches')\n","#         scipy.sparse.save_npz(save_path, new_matrix)\n","\n","#         # Clear lists to free memory\n","#         word_ids_list.clear()\n","#         context_ids_list.clear()\n","#         values_list.clear()\n","\n","\n","# # Run computation on A100 GPU\n","# compute_cooccurrence(token_ids)\n","\n","# # Load back the sparse matrix (Optional)\n","# loaded_matrix = scipy.sparse.load_npz(save_path)\n","\n","# print(f\"Co-occurrence matrix saved at: {save_path}\")\n","# print(f\"Non-zero co-occurrence pairs: {loaded_matrix.nnz}\")"],"metadata":{"id":"-v-ieUJHizAX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import torch\n","# import numpy as np\n","# import scipy.sparse\n","# import os\n","# from IPython.display import clear_output\n","\n","# # File paths\n","# save_path = \"/content/drive/MyDrive/DummyFolder/cooccurrence_matrix_50MB.npz\"\n","# checkpoint_path = \"/content/drive/MyDrive/DummyFolder/cooccurrence_checkpoint_50MB.txt\"\n","\n","# # Convert sentences to ID arrays\n","# token_ids = [[word_to_id[word] for word in sent] for sent in tokenized_sentences]\n","# token_ids = [torch.tensor(sent, dtype=torch.int32, device=device) for sent in token_ids]\n","\n","# # Vocabulary size\n","# vocab_size = len(word_to_id)\n","\n","# # Window size and batch size\n","# window_size = 8\n","# batch_size = 10000  # Adjust based on available memory\n","\n","# # Initialize storage variables\n","# word_ids_list, context_ids_list, values_list = [], [], []\n","\n","# # **Load checkpoint (if exists)**\n","# if os.path.exists(checkpoint_path):\n","#     with open(checkpoint_path, \"r\") as f:\n","#         last_processed_index = int(f.read().strip())\n","# else:\n","#     last_processed_index = 0\n","\n","# # **Recovery Logic**\n","# def recover_cooccurrence_data(file_path):\n","#     \"\"\"Attempt to recover data from a corrupted .npz file.\"\"\"\n","#     try:\n","#         with np.load(file_path, allow_pickle=True) as recovered_data:\n","#             recovered_matrix = scipy.sparse.coo_matrix(\n","#                 (recovered_data['data'], (recovered_data['row'], recovered_data['col'])),\n","#                 shape=(vocab_size, vocab_size)\n","#             )\n","#             print(\"✅ Recovery successful!\")\n","#             return recovered_matrix\n","#     except Exception as e:\n","#         print(f\"❌ Recovery failed: {e}\")\n","#         return scipy.sparse.coo_matrix((vocab_size, vocab_size))\n","\n","# # Load the matrix (recovery or initialize)\n","# if os.path.exists(save_path):\n","#     existing_matrix = recover_cooccurrence_data(save_path)\n","# else:\n","#     existing_matrix = scipy.sparse.coo_matrix((vocab_size, vocab_size))\n","\n","# def compute_cooccurrence(token_ids, start_index):\n","#     \"\"\"Compute co-occurrence matrix using a sliding window approach.\"\"\"\n","#     global word_ids_list, context_ids_list, values_list\n","\n","#     for idx in range(start_index, len(token_ids)):\n","#         sentence = token_ids[idx]\n","#         sent_len = sentence.shape[0]\n","\n","#         for i in range(sent_len):\n","#             word_id = sentence[i].item()\n","#             start = max(0, i - window_size)\n","#             end = min(sent_len, i + window_size + 1)\n","\n","#             for j in range(start, end):\n","#                 if i != j:\n","#                     context_id = sentence[j].item()\n","#                     distance = abs(j - i)\n","#                     increment = 1.0 / distance\n","#                     word_ids_list.append(word_id)\n","#                     context_ids_list.append(context_id)\n","#                     values_list.append(increment)\n","\n","#             # Save data when reaching batch size\n","#             if len(word_ids_list) >= batch_size:\n","#                 save_to_disk()\n","\n","#         # Periodic flushing & checkpointing\n","#         if idx % 1000 == 0:\n","#             save_to_disk()\n","#             with open(checkpoint_path, \"w\") as f:\n","#                 f.write(str(idx))  # Save last processed index\n","\n","#     # Final save & checkpoint update\n","#     save_to_disk()\n","#     with open(checkpoint_path, \"w\") as f:\n","#         f.write(str(len(token_ids)))  # Mark all sentences as processed\n","\n","# def save_to_disk():\n","#     \"\"\"Append co-occurrence data to an existing sparse matrix instead of overwriting.\"\"\"\n","#     global word_ids_list, context_ids_list, values_list, existing_matrix\n","\n","#     if word_ids_list:\n","#         # Convert current batch to sparse matrix\n","#         new_matrix = scipy.sparse.coo_matrix(\n","#             (values_list, (word_ids_list, context_ids_list)),\n","#             shape=(vocab_size, vocab_size)\n","#         )\n","\n","#         # Sum the new co-occurrence values with the existing ones\n","#         existing_matrix += new_matrix\n","\n","#         # Save updated matrix\n","#         clear_output()\n","#         print(f'Saved batch of {batch_size} words')\n","#         scipy.sparse.save_npz(save_path, existing_matrix)\n","\n","#         # Clear lists to free memory\n","#         word_ids_list.clear()\n","#         context_ids_list.clear()\n","#         values_list.clear()\n","\n","# # **Resume computation from last checkpoint**\n","# compute_cooccurrence(token_ids, last_processed_index)\n","\n","# # Load back the sparse matrix (Optional)\n","# loaded_matrix = scipy.sparse.load_npz(save_path)\n","\n","# print(f\"Co-occurrence matrix saved at: {save_path}\")\n","# print(f\"Non-zero co-occurrence pairs: {loaded_matrix.nnz}\")\n"],"metadata":{"id":"7CF35acW7Tzz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import scipy.sparse\n","import os\n","\n","# Make sure device is set to GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Vocabulary size and window size\n","vocab_size = len(word_to_id)\n","window_size = 8\n","token_ids = [[word_to_id[word] for word in sent] for sent in tokenized_sentences]\n","\n","# Convert to PyTorch GPU tensors\n","token_ids = [torch.tensor(sent, dtype=torch.int32, device=device) for sent in token_ids]\n","# File path for saving the matrix\n","save_path = \"/content/drive/MyDrive/DummyFolder/cooccurrence_matrix_50MBultimate.npz\"\n","\n","# Allocate a dense co-occurrence matrix on the GPU\n","# Note: This uses vocab_size x vocab_size elements. For a large vocabulary,\n","# ensure your GPU has enough memory.\n","cooccurrence_matrix = torch.zeros((vocab_size, vocab_size), dtype=torch.float32, device=device)\n","\n","def compute_cooccurrence_gpu(token_ids):\n","    \"\"\"Compute the co-occurrence matrix on the GPU using a dense matrix.\"\"\"\n","    for idx, sentence in enumerate(token_ids):\n","        sent_len = sentence.shape[0]\n","        for i in range(sent_len):\n","            word_id = sentence[i].item()  # Get the current word's ID\n","            # Determine the window boundaries\n","            start = max(0, i - window_size)\n","            end = min(sent_len, i + window_size + 1)\n","            for j in range(start, end):\n","                if i != j:\n","                    context_id = sentence[j].item()\n","                    distance = abs(j - i)\n","                    increment = 1.0 / distance\n","                    cooccurrence_matrix[word_id, context_id] += increment\n","\n","        # Print progress every 5000 sentences\n","        if idx % 5000 == 0:\n","            print(f\"Processed {idx} sentences...\")\n","\n","    # Convert the dense GPU matrix to CPU and then to a sparse matrix\n","    cooccurrence_cpu = cooccurrence_matrix.cpu().numpy()\n","    sparse_matrix = scipy.sparse.coo_matrix(cooccurrence_cpu)\n","    scipy.sparse.save_npz(save_path, sparse_matrix)\n","    print(f\"Final matrix saved at: {save_path}\")\n","\n","# Run computation using GPU-based token_ids\n","compute_cooccurrence_gpu(token_ids)\n","\n","# (Optional) Load and check the sparse matrix\n","loaded_matrix = scipy.sparse.load_npz(save_path)\n","print(f\"Non-zero co-occurrence pairs: {loaded_matrix.nnz}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UvKHdkkyG8nc","executionInfo":{"status":"ok","timestamp":1741780580443,"user_tz":-330,"elapsed":1619991,"user":{"displayName":"MultiDL","userId":"05750408194492004447"}},"outputId":"974d63e4-19ca-4510-efbc-e09035051025"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Processed 0 sentences...\n","Processed 5000 sentences...\n","Processed 10000 sentences...\n","Processed 15000 sentences...\n","Processed 20000 sentences...\n","Processed 25000 sentences...\n","Processed 30000 sentences...\n","Processed 35000 sentences...\n","Processed 40000 sentences...\n","Processed 45000 sentences...\n","Processed 50000 sentences...\n","Processed 55000 sentences...\n","Final matrix saved at: /content/drive/MyDrive/DummyFolder/cooccurrence_matrix_50MBultimate.npz\n","Non-zero co-occurrence pairs: 7559781\n"]}]},{"cell_type":"code","source":["# Load back the sparse matrix (Optional)\n","import scipy.sparse\n","save_path = \"/content/drive/MyDrive/DummyFolder/cooccurrence_matrix_50MBultimate.npz\"\n","\n","loaded_matrix = scipy.sparse.load_npz(save_path)\n","\n","print(f\"Co-occurrence matrix saved at: {save_path}\")\n","print(f\"Non-zero co-occurrence pairs: {loaded_matrix.nnz}\")"],"metadata":{"id":"6VRz53vC8rqe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741784971341,"user_tz":-330,"elapsed":3057,"user":{"displayName":"MultiDL","userId":"05750408194492004447"}},"outputId":"9fd2ce22-fe6e-4fea-c994-82472e7e2869"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Co-occurrence matrix saved at: /content/drive/MyDrive/DummyFolder/cooccurrence_matrix_50MBultimate.npz\n","Non-zero co-occurrence pairs: 7559781\n"]}]},{"cell_type":"code","source":["import scipy.sparse\n","import numpy as np\n","\n","# Load the sparse co-occurrence matrix\n","cooccur_sparse = scipy.sparse.load_npz(\"/content/drive/MyDrive/DummyFolder/cooccurrence_matrix_50MBultimate.npz\")\n","\n","# Convert to COO format (if not already)\n","cooccur_sparse = cooccur_sparse.tocoo()\n","\n","# Convert sparse matrix to dictionary: {(word_id, context_id): co-occurrence_count}\n","cooccurrence = {\n","    (int(i), int(j)): float(value)\n","    for i, j, value in zip(cooccur_sparse.row, cooccur_sparse.col, cooccur_sparse.data)\n","}\n","\n","# Print first 5 entries to verify\n","for k, v in list(cooccurrence.items())[:5]:\n","    print(f\"{k} -> {v}\")\n"],"metadata":{"id":"ifjUtACDVblx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741784981305,"user_tz":-330,"elapsed":7134,"user":{"displayName":"MultiDL","userId":"05750408194492004447"}},"outputId":"39d99b47-2792-4ce8-df09-88e6e1a3e503"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["(0, 0) -> 156.94528198242188\n","(0, 1) -> 134.8036346435547\n","(0, 2) -> 24.700002670288086\n","(0, 3) -> 14.749996185302734\n","(0, 4) -> 22.74166488647461\n"]}]},{"cell_type":"code","source":["import gc\n","\n","# Delete GPU tensors that are no longer needed.\n","del cooccurrence_matrix\n","del token_ids  # if you're done with these\n","\n","# Run garbage collection\n","gc.collect()\n","\n","# Clear PyTorch's cache\n","torch.cuda.empty_cache()\n","\n","print(\"GPU memory has been flushed.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SHYicxdMRBns","executionInfo":{"status":"ok","timestamp":1741781561090,"user_tz":-330,"elapsed":338,"user":{"displayName":"MultiDL","userId":"05750408194492004447"}},"outputId":"8450ad6a-e9c9-4431-e031-e8901cedd395"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU memory has been flushed.\n"]}]},{"cell_type":"code","source":["# total_values = cooccur_sparse.data.sum()\n","# print(f\"Total co-occurrence values: {total_values}\")\n","# unique_word_ids = np.unique(cooccur_sparse.row)\n","# total_word_ids = len(unique_word_ids)\n","# print(f\"Total unique word IDs: {total_word_ids}\")\n","# unique_context_ids = np.unique(cooccur_sparse.col)\n","# total_context_ids = len(unique_context_ids)\n","# print(f\"Total unique context IDs: {total_context_ids}\")\n","# total_nonzero_pairs = cooccur_sparse.nnz\n","# print(f\"Total non-zero co-occurrence pairs: {total_nonzero_pairs}\")\n"],"metadata":{"id":"u36d07QRW1di"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RX0Dw_oNMc2s"},"outputs":[],"source":["# import scipy.sparse\n","\n","# scipy.sparse.save_npz(\"/content/drive/MyDrive/DummyFolder/cooccurrence_matrix.npz\", cooccur_sparse)"]},{"cell_type":"code","source":[],"metadata":{"id":"tHdvAy62Rok_"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KpRiHQXmTI29"},"outputs":[],"source":["# print(cooccurrence)\n","# print(cooccur_matrix)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qeAWU7ITZ-N3"},"outputs":[],"source":["# print(np.sum(cooccur_matrix == 0) / np.prod(cooccur_matrix.shape))"]},{"cell_type":"markdown","metadata":{"id":"EbgJa58CWluI"},"source":["# **GloVe Model Preparation and Execution**"]},{"cell_type":"markdown","metadata":{"id":"ybQG9fWIWx1R"},"source":["**Initialization**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jdSDdGec4gOi"},"outputs":[],"source":["# # Initialize GloVe Model Parameters\n","\n","# embedding_dim = 50  # Dimensionality of embeddings\n","# # Word and context vectors (random initialization between -0.5 and 0.5)\n","# W = np.random.rand(vocab_size, embedding_dim) - 0.5\n","# W_context = np.random.rand(vocab_size, embedding_dim) - 0.5\n","# # Bias terms for words and contexts\n","# # bias = np.random.rand(vocab_size) - 0.5\n","# # bias_context = np.random.rand(vocab_size) - 0.5\n","# bias = np.zeros(vocab_size)\n","# bias_context = np.zeros(vocab_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IcneCqAVDfYh"},"outputs":[],"source":["# print(bias)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"esl6tWjiD3gm"},"outputs":[],"source":["# print(W)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dBSxmEML4QSr"},"outputs":[],"source":["# # Define Hyperparameters & Weighting Function\n","\n","# epochs = 25\n","# learning_rate = 0.05\n","# x_max = 100  # Parameter for weighting function\n","# alpha = 0.75\n","\n","# def weighting(x):\n","#     \"\"\"Weighting function f(x) as used in the GloVe paper.\"\"\"\n","#     return (x / x_max)**alpha if x < x_max else 1.0"]},{"cell_type":"markdown","metadata":{"id":"0z_RxKglW6PO"},"source":["**Training Glove Model using SGD**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QjKY4kG74l3O"},"outputs":[],"source":["# # Training Loop using SGD\n","\n","# for epoch in range(epochs):\n","#     total_loss = 0.0\n","#     # Loop over each non-zero co-occurrence pair\n","#     for (i, j), Xij in cooccurrence.items():\n","#         # Weight for this pair\n","#         weight = weighting(Xij)\n","#         # Compute the inner product between word and context vectors\n","#         dot = np.dot(W[i], W_context[j])\n","#         # Compute the error (difference from log co-occurrence count)\n","#         cost = dot + bias[i] + bias_context[j] - math.log(Xij)\n","#         # Weighted error\n","#         weighted_cost = weight * cost\n","\n","#         # Accumulate loss (squared error)\n","#         total_loss += 0.5 * weighted_cost * cost\n","\n","#         # Compute gradients\n","#         grad_wi = weighted_cost * W_context[j]\n","#         grad_wj_context = weighted_cost * W[i]\n","\n","#         # Update the word and context vectors using SGD\n","#         W[i] -= learning_rate * grad_wi\n","#         W_context[j] -= learning_rate * grad_wj_context\n","\n","#         # Update the biases\n","#         # bias[i] -= learning_rate * weighted_cost\n","#         # bias_context[j] -= learning_rate * weighted_cost\n","\n","#         bias[i] -= (learning_rate * 0.1) * weighted_cost\n","#         bias_context[j] -= (learning_rate * 0.1) * weighted_cost\n","\n","#     print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss}\")"]},{"cell_type":"code","source":["# import torch\n","# import math\n","# import scipy.sparse  # if needed elsewhere\n","# import numpy as np\n","\n","# # Set device to GPU\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# # Assume word_to_id and cooccurrence are already defined.\n","# # vocab_size is defined as:\n","# vocab_size = len(word_to_id)\n","\n","# # GloVe hyperparameters\n","# embedding_dim = 50     # Dimensionality of embeddings\n","# epochs = 25\n","# learning_rate = 0.05\n","# x_max = 100.0          # For weighting function\n","# alpha = 0.75\n","\n","# def weighting(x):\n","#     \"\"\"Weighting function f(x) as used in the GloVe paper.\"\"\"\n","#     return (x / x_max)**alpha if x < x_max else 1.0\n","\n","# # Initialize model parameters as PyTorch tensors on the GPU.\n","# # We use torch.rand to initialize uniformly in [0,1) then subtract 0.5.\n","# W = torch.rand(vocab_size, embedding_dim, device=device) - 0.5\n","# W_context = torch.rand(vocab_size, embedding_dim, device=device) - 0.5\n","# bias = torch.zeros(vocab_size, device=device)\n","# bias_context = torch.zeros(vocab_size, device=device)\n","\n","# # Training loop using SGD (manual updates)\n","# for epoch in range(epochs):\n","#     total_loss = 0.0\n","#     # Iterate over each non-zero co-occurrence pair in the dictionary.\n","#     # cooccurrence is assumed to be a dict: {(i, j): Xij}\n","#     for (i, j), Xij in cooccurrence.items():\n","#         weight = weighting(Xij)\n","\n","#         # Compute the inner product between the word vector and context vector.\n","#         dot = torch.dot(W[i], W_context[j])\n","\n","#         # Compute the error from the log co-occurrence count.\n","#         cost = dot + bias[i] + bias_context[j] - torch.log(torch.tensor(float(Xij), device=device))\n","#         weighted_cost = weight * cost\n","\n","#         # Accumulate loss (convert the scalar to a Python float)\n","#         total_loss += 0.5 * (weighted_cost * cost).item()\n","\n","#         # Compute gradients (all operations occur on GPU)\n","#         grad_wi = weighted_cost * W_context[j]\n","#         grad_wj_context = weighted_cost * W[i]\n","\n","#         # Update parameters using SGD\n","#         W[i] = W[i] - learning_rate * grad_wi\n","#         W_context[j] = W_context[j] - learning_rate * grad_wj_context\n","#         bias[i] = bias[i] - (learning_rate * 0.1) * weighted_cost\n","#         bias_context[j] = bias_context[j] - (learning_rate * 0.1) * weighted_cost\n","\n","#     print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"Fhrf5A6gRgGa","executionInfo":{"status":"error","timestamp":1741787228419,"user_tz":-330,"elapsed":2235796,"user":{"displayName":"MultiDL","userId":"05750408194492004447"}},"outputId":"1d22edec-b03b-4f62-cca6-4c0ea4b346c4"},"execution_count":14,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-d245ce6ecd98>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mW_context\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW_context\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad_wj_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mbias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweighted_cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mbias_context\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbias_context\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweighted_cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{epochs}, Loss: {total_loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["import torch\n","import math\n","import numpy as np\n","\n","# Set device to GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Assume word_to_id is already defined and cooccurrence is a dict: {(i, j): Xij}\n","vocab_size = len(word_to_id)\n","\n","# GloVe hyperparameters\n","embedding_dim = 150     # Dimensionality of embeddings\n","epochs = 150\n","learning_rate = 0.05\n","x_max = 100.0          # For weighting function\n","alpha = 0.75\n","\n","def weighting(x):\n","    \"\"\"Vectorized weighting function f(x) as used in the GloVe paper.\"\"\"\n","    return torch.where(x < x_max, (x / x_max)**alpha, torch.ones_like(x))\n","\n","# Initialize model parameters as PyTorch tensors on the GPU.\n","W = torch.rand(vocab_size, embedding_dim, device=device) - 0.5\n","W_context = torch.rand(vocab_size, embedding_dim, device=device) - 0.5\n","bias = torch.zeros(vocab_size, device=device)\n","bias_context = torch.zeros(vocab_size, device=device)\n","\n","# --- Preprocess co-occurrence dictionary into tensors ---\n","# Convert dictionary keys and values to lists and then to tensors.\n","pairs = list(cooccurrence.keys())           # List of tuples (i, j)\n","values = list(cooccurrence.values())          # List of Xij values\n","\n","# Convert to tensors (on GPU)\n","pairs_tensor = torch.tensor(pairs, dtype=torch.long, device=device)  # shape (num_pairs, 2)\n","Xij_tensor = torch.tensor(values, dtype=torch.float32, device=device)  # shape (num_pairs,)\n","\n","# Pre-compute log(Xij)\n","log_Xij_tensor = torch.log(Xij_tensor)\n","\n","num_pairs = pairs_tensor.shape[0]\n","batch_size = 100000  # Adjust batch size based on your GPU memory\n","\n","# --- Vectorized Training Loop ---\n","for epoch in range(epochs):\n","    total_loss = 0.0\n","\n","    # Shuffle indices for each epoch\n","    perm = torch.randperm(num_pairs, device=device)\n","    pairs_shuffled = pairs_tensor[perm]\n","    Xij_shuffled = Xij_tensor[perm]\n","    log_Xij_shuffled = log_Xij_tensor[perm]\n","\n","    # Process in batches\n","    for start in range(0, num_pairs, batch_size):\n","        end = min(start + batch_size, num_pairs)\n","        batch_pairs = pairs_shuffled[start:end]\n","        batch_Xij = Xij_shuffled[start:end]\n","        batch_log_Xij = log_Xij_shuffled[start:end]\n","\n","        # Separate word indices and context indices\n","        i_batch = batch_pairs[:, 0]\n","        j_batch = batch_pairs[:, 1]\n","\n","        # Vectorized dot product for batch: (batch_size,)\n","        dot = torch.sum(W[i_batch] * W_context[j_batch], dim=1)\n","\n","        # Compute cost for the batch\n","        cost = dot + bias[i_batch] + bias_context[j_batch] - batch_log_Xij\n","        weight = weighting(batch_Xij)\n","        weighted_cost = weight * cost\n","\n","        # Accumulate loss\n","        loss = 0.5 * torch.sum(weighted_cost * cost)\n","        total_loss += loss.item()\n","\n","        # Compute gradients for batch (each is (batch_size, embedding_dim))\n","        grad_wi = weighted_cost.unsqueeze(1) * W_context[j_batch]\n","        grad_wj_context = weighted_cost.unsqueeze(1) * W[i_batch]\n","\n","        # Now update parameters using scatter-add style accumulation.\n","        # Using index_put_ with accumulate=True applies the updates to repeated indices.\n","        W.index_put_((i_batch,), -learning_rate * grad_wi, accumulate=True)\n","        W_context.index_put_((j_batch,), -learning_rate * grad_wj_context, accumulate=True)\n","        bias.index_put_((i_batch,), -learning_rate * 0.1 * weighted_cost, accumulate=True)\n","        bias_context.index_put_((j_batch,), -learning_rate * 0.1 * weighted_cost, accumulate=True)\n","\n","    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p70PR7wWnBaV","executionInfo":{"status":"ok","timestamp":1741793544834,"user_tz":-330,"elapsed":33945,"user":{"displayName":"MultiDL","userId":"05750408194492004447"}},"outputId":"fc153e97-869e-4d14-bf2a-2117c540dc16"},"execution_count":120,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/150, Loss: 359538.4501953125\n","Epoch 2/150, Loss: 205648.4522705078\n","Epoch 3/150, Loss: 173963.54553222656\n","Epoch 4/150, Loss: 155303.9610595703\n","Epoch 5/150, Loss: 142313.28381347656\n","Epoch 6/150, Loss: 132445.70178222656\n","Epoch 7/150, Loss: 124488.76147460938\n","Epoch 8/150, Loss: 117904.81372070312\n","Epoch 9/150, Loss: 112364.63482666016\n","Epoch 10/150, Loss: 107651.12829589844\n","Epoch 11/150, Loss: 103459.58471679688\n","Epoch 12/150, Loss: 99780.87908935547\n","Epoch 13/150, Loss: 96502.45373535156\n","Epoch 14/150, Loss: 93519.30456542969\n","Epoch 15/150, Loss: 90862.00756835938\n","Epoch 16/150, Loss: 88392.67272949219\n","Epoch 17/150, Loss: 86139.39196777344\n","Epoch 18/150, Loss: 84067.23706054688\n","Epoch 19/150, Loss: 82139.38116455078\n","Epoch 20/150, Loss: 80385.11706542969\n","Epoch 21/150, Loss: 78688.20849609375\n","Epoch 22/150, Loss: 77137.19500732422\n","Epoch 23/150, Loss: 75648.0005493164\n","Epoch 24/150, Loss: 74310.01959228516\n","Epoch 25/150, Loss: 72967.24151611328\n","Epoch 26/150, Loss: 71778.56427001953\n","Epoch 27/150, Loss: 70561.94799804688\n","Epoch 28/150, Loss: 69443.16796875\n","Epoch 29/150, Loss: 68415.87451171875\n","Epoch 30/150, Loss: 67387.4185180664\n","Epoch 31/150, Loss: 66433.73413085938\n","Epoch 32/150, Loss: 65529.29992675781\n","Epoch 33/150, Loss: 64638.153076171875\n","Epoch 34/150, Loss: 63812.484130859375\n","Epoch 35/150, Loss: 63002.949768066406\n","Epoch 36/150, Loss: 62213.556091308594\n","Epoch 37/150, Loss: 61483.225158691406\n","Epoch 38/150, Loss: 60763.150451660156\n","Epoch 39/150, Loss: 60060.46423339844\n","Epoch 40/150, Loss: 59442.54409790039\n","Epoch 41/150, Loss: 58772.4651184082\n","Epoch 42/150, Loss: 58159.627380371094\n","Epoch 43/150, Loss: 57537.77600097656\n","Epoch 44/150, Loss: 56977.5234375\n","Epoch 45/150, Loss: 56464.85137939453\n","Epoch 46/150, Loss: 55880.953552246094\n","Epoch 47/150, Loss: 55341.5341796875\n","Epoch 48/150, Loss: 54846.541259765625\n","Epoch 49/150, Loss: 54357.29766845703\n","Epoch 50/150, Loss: 53890.13687133789\n","Epoch 51/150, Loss: 53445.872131347656\n","Epoch 52/150, Loss: 52960.97979736328\n","Epoch 53/150, Loss: 52556.17123413086\n","Epoch 54/150, Loss: 52121.505859375\n","Epoch 55/150, Loss: 51669.180114746094\n","Epoch 56/150, Loss: 51310.04187011719\n","Epoch 57/150, Loss: 50924.32391357422\n","Epoch 58/150, Loss: 50523.903900146484\n","Epoch 59/150, Loss: 50129.11700439453\n","Epoch 60/150, Loss: 49815.50622558594\n","Epoch 61/150, Loss: 49438.27410888672\n","Epoch 62/150, Loss: 49071.92248535156\n","Epoch 63/150, Loss: 48772.406311035156\n","Epoch 64/150, Loss: 48420.550231933594\n","Epoch 65/150, Loss: 48124.33380126953\n","Epoch 66/150, Loss: 47803.961669921875\n","Epoch 67/150, Loss: 47537.516204833984\n","Epoch 68/150, Loss: 47194.04766845703\n","Epoch 69/150, Loss: 46865.89862060547\n","Epoch 70/150, Loss: 46601.26647949219\n","Epoch 71/150, Loss: 46330.73550415039\n","Epoch 72/150, Loss: 46074.279235839844\n","Epoch 73/150, Loss: 45791.04879760742\n","Epoch 74/150, Loss: 45552.979400634766\n","Epoch 75/150, Loss: 45318.19201660156\n","Epoch 76/150, Loss: 45057.10418701172\n","Epoch 77/150, Loss: 44813.68518066406\n","Epoch 78/150, Loss: 44547.73553466797\n","Epoch 79/150, Loss: 44361.77389526367\n","Epoch 80/150, Loss: 44078.72784423828\n","Epoch 81/150, Loss: 43846.17590332031\n","Epoch 82/150, Loss: 43599.204528808594\n","Epoch 83/150, Loss: 43447.92068481445\n","Epoch 84/150, Loss: 43198.40985107422\n","Epoch 85/150, Loss: 43039.532287597656\n","Epoch 86/150, Loss: 42794.993103027344\n","Epoch 87/150, Loss: 42617.12631225586\n","Epoch 88/150, Loss: 42426.62469482422\n","Epoch 89/150, Loss: 42206.49172973633\n","Epoch 90/150, Loss: 42039.490234375\n","Epoch 91/150, Loss: 41800.026123046875\n","Epoch 92/150, Loss: 41724.197814941406\n","Epoch 93/150, Loss: 41464.320068359375\n","Epoch 94/150, Loss: 41287.09637451172\n","Epoch 95/150, Loss: 41092.684661865234\n","Epoch 96/150, Loss: 40929.35076904297\n","Epoch 97/150, Loss: 40766.723388671875\n","Epoch 98/150, Loss: 40583.884674072266\n","Epoch 99/150, Loss: 40425.84280395508\n","Epoch 100/150, Loss: 40285.535552978516\n","Epoch 101/150, Loss: 40098.68762207031\n","Epoch 102/150, Loss: 40094.02865600586\n","Epoch 103/150, Loss: 39771.19140625\n","Epoch 104/150, Loss: 39626.23422241211\n","Epoch 105/150, Loss: 39490.33660888672\n","Epoch 106/150, Loss: 39383.500579833984\n","Epoch 107/150, Loss: 39230.776275634766\n","Epoch 108/150, Loss: 39119.65335083008\n","Epoch 109/150, Loss: 38930.05355834961\n","Epoch 110/150, Loss: 38796.890869140625\n","Epoch 111/150, Loss: 38665.64324951172\n","Epoch 112/150, Loss: 38509.22607421875\n","Epoch 113/150, Loss: 38435.397857666016\n","Epoch 114/150, Loss: 38357.76834106445\n","Epoch 115/150, Loss: 38161.46725463867\n","Epoch 116/150, Loss: 38029.03271484375\n","Epoch 117/150, Loss: 37901.82586669922\n","Epoch 118/150, Loss: 37767.44741821289\n","Epoch 119/150, Loss: 37648.970123291016\n","Epoch 120/150, Loss: 37537.93029785156\n","Epoch 121/150, Loss: 37444.2883605957\n","Epoch 122/150, Loss: 37301.136657714844\n","Epoch 123/150, Loss: 37187.49450683594\n","Epoch 124/150, Loss: 37098.94689941406\n","Epoch 125/150, Loss: 36991.72085571289\n","Epoch 126/150, Loss: 36859.850494384766\n","Epoch 127/150, Loss: 36743.8244934082\n","Epoch 128/150, Loss: 36622.41860961914\n","Epoch 129/150, Loss: 36535.125427246094\n","Epoch 130/150, Loss: 36447.0632019043\n","Epoch 131/150, Loss: 36328.135650634766\n","Epoch 132/150, Loss: 36243.49951171875\n","Epoch 133/150, Loss: 36196.37322998047\n","Epoch 134/150, Loss: 36041.0390625\n","Epoch 135/150, Loss: 36003.757751464844\n","Epoch 136/150, Loss: 35834.2278137207\n","Epoch 137/150, Loss: 35739.8512878418\n","Epoch 138/150, Loss: 35676.114807128906\n","Epoch 139/150, Loss: 35569.318450927734\n","Epoch 140/150, Loss: 35539.72021484375\n","Epoch 141/150, Loss: 35416.90270996094\n","Epoch 142/150, Loss: 35366.7375793457\n","Epoch 143/150, Loss: 35268.61145019531\n","Epoch 144/150, Loss: 35142.376892089844\n","Epoch 145/150, Loss: 35045.924377441406\n","Epoch 146/150, Loss: 34947.68963623047\n","Epoch 147/150, Loss: 34871.284729003906\n","Epoch 148/150, Loss: 34756.050354003906\n","Epoch 149/150, Loss: 34725.47497558594\n","Epoch 150/150, Loss: 34641.667388916016\n"]}]},{"cell_type":"markdown","source":["***Normal (Gaussian) initialization***"],"metadata":{"id":"6tvXFTtgzDQ6"}},{"cell_type":"code","source":["# import torch\n","# import math\n","# import numpy as np\n","\n","# # Set device to GPU\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# # Assume word_to_id is already defined and cooccurrence is a dict: {(i, j): Xij}\n","# vocab_size = len(word_to_id)\n","\n","# # GloVe hyperparameters\n","# embedding_dim = 150     # Dimensionality of embeddings\n","# epochs = 100\n","# learning_rate = 0.05\n","# x_max = 100.0          # For weighting function\n","# alpha = 0.75\n","\n","# def weighting(x):\n","#     \"\"\"Vectorized weighting function f(x) as used in the GloVe paper.\"\"\"\n","#     return torch.where(x < x_max, (x / x_max)**alpha, torch.ones_like(x))\n","\n","# # Initialize model parameters as PyTorch tensors on the GPU.\n","\n","# # Create empty tensors for word embeddings and context embeddings.\n","# W = torch.empty(vocab_size, embedding_dim, device=device)\n","# W_context = torch.empty(vocab_size, embedding_dim, device=device)\n","\n","# # Apply Normal (Gaussian) initialization with mean=0.0 and std=0.01\n","# torch.nn.init.normal_(W, mean=0.0, std=0.01)\n","# torch.nn.init.normal_(W_context, mean=0.0, std=0.01)\n","\n","# # Initialize bias terms (commonly to zeros)\n","# bias = torch.zeros(vocab_size, device=device)\n","# bias_context = torch.zeros(vocab_size, device=device)\n","\n","# print(\"Normal (Gaussian) initialization complete for W and W_context.\")\n","\n","# # --- Preprocess co-occurrence dictionary into tensors ---\n","# # Convert dictionary keys and values to lists and then to tensors.\n","# pairs = list(cooccurrence.keys())           # List of tuples (i, j)\n","# values = list(cooccurrence.values())          # List of Xij values\n","\n","# # Convert to tensors (on GPU)\n","# pairs_tensor = torch.tensor(pairs, dtype=torch.long, device=device)  # shape (num_pairs, 2)\n","# Xij_tensor = torch.tensor(values, dtype=torch.float32, device=device)  # shape (num_pairs,)\n","\n","# # Pre-compute log(Xij)\n","# log_Xij_tensor = torch.log(Xij_tensor)\n","\n","# num_pairs = pairs_tensor.shape[0]\n","# batch_size = 100000  # Adjust batch size based on your GPU memory\n","\n","# # --- Vectorized Training Loop ---\n","# for epoch in range(epochs):\n","#     total_loss = 0.0\n","\n","#     # Shuffle indices for each epoch\n","#     perm = torch.randperm(num_pairs, device=device)\n","#     pairs_shuffled = pairs_tensor[perm]\n","#     Xij_shuffled = Xij_tensor[perm]\n","#     log_Xij_shuffled = log_Xij_tensor[perm]\n","\n","#     # Process in batches\n","#     for start in range(0, num_pairs, batch_size):\n","#         end = min(start + batch_size, num_pairs)\n","#         batch_pairs = pairs_shuffled[start:end]\n","#         batch_Xij = Xij_shuffled[start:end]\n","#         batch_log_Xij = log_Xij_shuffled[start:end]\n","\n","#         # Separate word indices and context indices\n","#         i_batch = batch_pairs[:, 0]\n","#         j_batch = batch_pairs[:, 1]\n","\n","#         # Vectorized dot product for batch: (batch_size,)\n","#         dot = torch.sum(W[i_batch] * W_context[j_batch], dim=1)\n","\n","#         # Compute cost for the batch\n","#         cost = dot + bias[i_batch] + bias_context[j_batch] - batch_log_Xij\n","#         weight = weighting(batch_Xij)\n","#         weighted_cost = weight * cost\n","\n","#         # Accumulate loss\n","#         loss = 0.5 * torch.sum(weighted_cost * cost)\n","#         total_loss += loss.item()\n","\n","#         # Compute gradients for batch (each is (batch_size, embedding_dim))\n","#         grad_wi = weighted_cost.unsqueeze(1) * W_context[j_batch]\n","#         grad_wj_context = weighted_cost.unsqueeze(1) * W[i_batch]\n","\n","#         # Now update parameters using scatter-add style accumulation.\n","#         # Using index_put_ with accumulate=True applies the updates to repeated indices.\n","#         W.index_put_((i_batch,), -learning_rate * grad_wi, accumulate=True)\n","#         W_context.index_put_((j_batch,), -learning_rate * grad_wj_context, accumulate=True)\n","#         bias.index_put_((i_batch,), -learning_rate * 0.1 * weighted_cost, accumulate=True)\n","#         bias_context.index_put_((j_batch,), -learning_rate * 0.1 * weighted_cost, accumulate=True)\n","\n","#     print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss}\")\n"],"metadata":{"id":"ZNbl7HkWyw7o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-s-zwdawXD4G"},"source":["**Save Generated Embeddings in txt Format**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zAy0kWQq4ow5"},"outputs":[],"source":["# # Combine and Save the Final Embeddings in txt format\n","\n","# # A common practice is to sum (or average) the word and context embeddings\n","# final_embeddings = W + W_context\n","\n","# # Save embeddings to a text file (each line: word followed by its embedding)\n","# with open('/content/drive/MyDrive/DummyFolder/HNVec100MB.txt', 'w', encoding='utf-8') as f:\n","#     for word, i in word_to_id.items():\n","#         vector = ' '.join(map(str, final_embeddings[i]))\n","#         f.write(f\"{word} {vector}\\n\")\n","\n","# print(\"Training complete. Embeddings saved to 'HNVec.txt'.\")"]},{"cell_type":"code","source":["# Combine and Save the Final Embeddings in txt format\n","\n","# A common practice is to sum (or average) the word and context embeddings\n","final_embeddings = (W + W_context).cpu()  # Move to CPU\n","\n","# Save embeddings to a text file (each line: word followed by its embedding)\n","output_file = '/content/drive/MyDrive/DummyFolder/HNVec50MB.txt'\n","with open(output_file, 'w', encoding='utf-8') as f:\n","    for word, i in word_to_id.items():\n","        # Convert tensor to list for cleaner formatting\n","        vector = ' '.join(map(str, final_embeddings[i].tolist()))\n","        f.write(f\"{word} {vector}\\n\")\n","\n","print(f\"Training complete. Embeddings saved to '{output_file}'.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eRELpm0OSzLo","executionInfo":{"status":"ok","timestamp":1741793561051,"user_tz":-330,"elapsed":7144,"user":{"displayName":"MultiDL","userId":"05750408194492004447"}},"outputId":"117c41d2-194e-49ae-b162-78d01367eeea"},"execution_count":121,"outputs":[{"output_type":"stream","name":"stdout","text":["Training complete. Embeddings saved to '/content/drive/MyDrive/DummyFolder/HNVec50MB.txt'.\n"]}]},{"cell_type":"markdown","metadata":{"id":"DYhi3mshXRWR"},"source":["**Save Glove model in pkl format**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3VXQQIKa69Em"},"outputs":[],"source":["# import pickle\n","\n","# # Save the embeddings dictionary as a .pkl file\n","# model_data = {\n","#     \"word_to_id\": word_to_id,\n","#     \"id_to_word\": {i: word for word, i in word_to_id.items()},\n","#     \"embeddings\": final_embeddings\n","# }\n","\n","# with open(\"/content/drive/MyDrive/DummyFolder/HNVec100MB.pkl\", \"wb\") as f:\n","#     pickle.dump(model_data, f)\n","\n","# print(\"Model saved as 'HNVec100MB.pkl'.\")\n"]},{"cell_type":"code","source":["import pickle\n","\n","# (Optional) Ensure final_embeddings is on CPU and convert to numpy if it's a tensor.\n","final_embeddings = final_embeddings.cpu().detach().numpy()\n","\n","# Save the embeddings dictionary as a .pkl file\n","model_data = {\n","    \"word_to_id\": word_to_id,\n","    \"id_to_word\": {i: word for word, i in word_to_id.items()},\n","    \"embeddings\": final_embeddings\n","}\n","\n","model_output = '/content/drive/MyDrive/DummyFolder/HNVec50MB.pkl'\n","with open(model_output, \"wb\") as f:\n","    pickle.dump(model_data, f)\n","\n","print(f\"Model saved to '{model_output}'.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Icvo5kPUTDk4","executionInfo":{"status":"ok","timestamp":1741794001167,"user_tz":-330,"elapsed":138,"user":{"displayName":"MultiDL","userId":"05750408194492004447"}},"outputId":"03410fab-794b-4a3a-92a3-1c43d5aaf09d"},"execution_count":130,"outputs":[{"output_type":"stream","name":"stdout","text":["Model saved to '/content/drive/MyDrive/DummyFolder/HNVec50MB.pkl'.\n"]}]},{"cell_type":"markdown","metadata":{"id":"YYKdqYfnXbKV"},"source":["# **Steps to import and use Glove Model**"]},{"cell_type":"code","execution_count":131,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":69,"status":"ok","timestamp":1741794006826,"user":{"displayName":"MultiDL","userId":"05750408194492004447"},"user_tz":-330},"id":"oSt1i8307IJo","outputId":"43befee4-26b2-4661-dd09-296277dd982b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Embedding for 'मेट्रो': [ 1.1470363   0.3918727  -0.07192533 -0.39991024 -0.9962837   0.16012998\n","  0.03748996  0.67268586  0.6152403  -0.16711852 -0.5713951   0.2055284\n"," -0.39232767  0.40831295  0.31375968 -0.25927767 -0.5682203   0.23190357\n"," -1.0165441  -0.13586225  0.07439667 -0.22753613 -0.5519527  -0.2954046\n","  0.65655345  0.59378463  0.6932719   0.19834243 -0.08218037 -0.4319578\n","  0.35499805  1.2892915   0.27267823  0.1628054   0.15434489  0.37593234\n"," -0.07960588  0.26832932  0.61413467 -0.41428658  0.33253273 -0.14717603\n"," -0.43292356 -0.5595272   0.8874601  -0.37730634  0.6848813   0.41775966\n"," -0.34516618 -0.3525604  -0.21011913 -0.17072406  0.8410872   0.11959081\n","  0.40918326 -0.81320655  0.7277419  -0.02005552 -0.8379956  -0.46914095\n","  0.5946795  -0.81984437 -0.06028949  0.16553348  0.18494162  0.03038067\n"," -0.6446216   0.4227677  -0.05241515 -0.3086492   0.38709456  0.4338052\n","  0.06849141 -0.32135004  0.06714053 -0.9235772  -0.2941364  -0.24309906\n","  0.00329646  0.41395935 -0.28755757 -0.08601052  0.7712501   0.59531295\n"," -0.40242788  0.02991149 -0.19937009  0.12296359 -0.20520844  0.12250844\n"," -0.14160079  0.35619247  0.56480795  0.3812067   0.6289303  -1.2442935\n","  0.39723298 -0.9257138   0.12755266  0.16923274  0.8044126  -0.07421415\n","  1.4682317   0.35287842  0.03720723  0.9541478   0.42702097  1.1323342\n"," -0.03802183 -0.02249968 -0.2957042   0.17729494  0.19433284 -0.46039432\n"," -0.09023936 -0.19182657 -0.16244197 -0.5863048  -0.3508864   0.39128062\n","  0.40414298  0.05927828  0.18980129 -0.5444443   0.61803323 -0.8891659\n","  0.2842076  -0.43015525 -0.38292003  0.52693206  0.01695773 -0.5169021\n","  0.6807959   0.44867468 -0.6148047  -0.4276333   0.28212702  0.03621072\n","  0.7206731  -1.2609639  -0.12477031  0.15325315  0.62914395 -0.03836107\n"," -0.2517453   0.2050983  -0.72253764 -0.50234175 -0.28115326 -0.73019046]\n"]}],"source":["# Steps to import the model and get corresponding vector embedding using pkl\n","\n","import pickle\n","import numpy as np\n","\n","# Load the saved model\n","with open(\"/content/drive/MyDrive/DummyFolder/HNVec50MB.pkl\", \"rb\") as f:\n","    model_data = pickle.load(f)\n","\n","# Extract components\n","word_to_id = model_data[\"word_to_id\"]\n","id_to_word = model_data[\"id_to_word\"]\n","embeddings = model_data[\"embeddings\"]\n","\n","# Example: Get the embedding of a word\n","word = \"मेट्रो\"\n","if word in word_to_id:\n","    word_embedding = embeddings[word_to_id[word]]\n","    print(f\"Embedding for '{word}': {word_embedding}\")\n","else:\n","    print(f\"Word '{word}' not found in vocabulary.\")\n"]},{"cell_type":"code","execution_count":132,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4106,"status":"ok","timestamp":1741794022530,"user":{"displayName":"MultiDL","userId":"05750408194492004447"},"user_tz":-330},"id":"3WB69Z787tS5","outputId":"42558c6a-3f00-4de8-ed63-05cd92911c18"},"outputs":[{"output_type":"stream","name":"stdout","text":["Embedding for 'मेट्रो': [ 1.1470363   0.3918727  -0.07192533 -0.39991024 -0.9962837   0.16012998\n","  0.03748996  0.67268586  0.6152403  -0.16711852 -0.5713951   0.2055284\n"," -0.39232767  0.40831295  0.31375968 -0.25927767 -0.5682203   0.23190357\n"," -1.0165441  -0.13586225  0.07439667 -0.22753613 -0.5519527  -0.2954046\n","  0.65655345  0.59378463  0.6932719   0.19834243 -0.08218037 -0.4319578\n","  0.35499805  1.2892915   0.27267823  0.1628054   0.15434489  0.37593234\n"," -0.07960588  0.26832932  0.61413467 -0.41428658  0.33253273 -0.14717603\n"," -0.43292356 -0.5595272   0.8874601  -0.37730634  0.6848813   0.41775966\n"," -0.34516618 -0.3525604  -0.21011913 -0.17072406  0.8410872   0.11959081\n","  0.40918326 -0.81320655  0.7277419  -0.02005552 -0.8379956  -0.46914095\n","  0.5946795  -0.81984437 -0.06028949  0.16553348  0.18494162  0.03038067\n"," -0.6446216   0.4227677  -0.05241515 -0.3086492   0.38709456  0.4338052\n","  0.06849141 -0.32135004  0.06714053 -0.9235772  -0.2941364  -0.24309906\n","  0.00329646  0.41395935 -0.28755757 -0.08601052  0.7712501   0.59531295\n"," -0.40242788  0.02991149 -0.19937009  0.12296359 -0.20520844  0.12250844\n"," -0.14160079  0.35619247  0.56480795  0.3812067   0.6289303  -1.2442935\n","  0.39723298 -0.9257138   0.12755266  0.16923274  0.8044126  -0.07421415\n","  1.4682317   0.35287842  0.03720723  0.9541478   0.42702097  1.1323342\n"," -0.03802183 -0.02249968 -0.2957042   0.17729494  0.19433284 -0.46039432\n"," -0.09023936 -0.19182657 -0.16244197 -0.5863048  -0.3508864   0.39128062\n","  0.40414298  0.05927828  0.18980129 -0.5444443   0.61803323 -0.8891659\n","  0.2842076  -0.43015525 -0.38292003  0.52693206  0.01695773 -0.5169021\n","  0.6807959   0.44867468 -0.6148047  -0.4276333   0.28212702  0.03621072\n","  0.7206731  -1.2609639  -0.12477031  0.15325315  0.62914395 -0.03836107\n"," -0.2517453   0.2050983  -0.72253764 -0.50234175 -0.28115326 -0.73019046]\n"]}],"source":["# Steps to get corresponding vector embedding using txt file\n","word_embeddings = {}\n","with open(\"/content/drive/MyDrive/DummyFolder/HNVec50MB.txt\", \"r\", encoding=\"utf-8\") as f:\n","    for line in f:\n","        values = line.split()\n","        word = values[0]\n","        vector = np.array(values[1:], dtype=\"float32\")\n","        word_embeddings[word] = vector\n","\n","# Get vector for a word\n","demo_word = \"मेट्रो\"\n","print(f\"Embedding for '{demo_word}': {word_embeddings[demo_word]}\")\n","# print(word_embeddings[\"भारत\"])  # Example vector for \"भारत\""]},{"cell_type":"code","execution_count":129,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23714,"status":"ok","timestamp":1741793776450,"user":{"displayName":"MultiDL","userId":"05750408194492004447"},"user_tz":-330},"id":"nExWAZd88XsS","outputId":"6229d410-2262-4132-b93d-43e4adfc1f34"},"outputs":[{"output_type":"stream","name":"stdout","text":["[('दिल्ली', 0.44579363), ('डांस', 0.43859777), ('रेल', 0.36082327), ('सफर', 0.35479024), ('कियाशानदार', 0.34631544)]\n"]}],"source":["# Steps to find similar words\n","# We can find similar words using cosine similarity.\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","def find_similar(word, top_n=5):\n","    if word not in word_embeddings:\n","        return \"Word not in vocabulary\"\n","\n","    word_vec = word_embeddings[word].reshape(1, -1)\n","    similarities = {w: cosine_similarity(word_vec, v.reshape(1, -1))[0][0]\n","                    for w, v in word_embeddings.items()}\n","    sorted_words = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[1:top_n+1]\n","    return sorted_words\n","\n","print(find_similar(\"मेट्रो\"))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o3uP_vw08jHI"},"outputs":[],"source":["# print(W)"]},{"cell_type":"code","source":["import numpy as np\n","\n","def cosine_similarity(vec1, vec2):\n","    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n","    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n","\n","def word_analogy(word_a, word_b, word_c, final_embeddings, word_to_id, id_to_word, top_n=5):\n","    \"\"\"\n","    Solves the analogy: word_a is to word_b as word_c is to ?\n","\n","    For example, if word_a = \"king\", word_b = \"queen\", word_c = \"man\",\n","    then the function should ideally return \"woman\" among the top results.\n","    \"\"\"\n","    # Ensure all words are in the vocabulary.\n","    for word in [word_a, word_b, word_c]:\n","        if word not in word_to_id:\n","            print(f\"Word '{word}' not found in the vocabulary.\")\n","            return None\n","\n","    # Compute the target vector: vec(word_b) - vec(word_a) + vec(word_c)\n","    vec_a = final_embeddings[word_to_id[word_a]]\n","    vec_b = final_embeddings[word_to_id[word_b]]\n","    vec_c = final_embeddings[word_to_id[word_c]]\n","    target_vec = vec_b - vec_a + vec_c\n","\n","    # Normalize the target vector.\n","    target_vec = target_vec / np.linalg.norm(target_vec)\n","\n","    # Compute cosine similarity with all words in the vocabulary.\n","    similarities = {}\n","    for word, idx in word_to_id.items():\n","        # Optionally skip the input words\n","        if word in [word_a, word_b, word_c]:\n","            continue\n","        vec = final_embeddings[idx]\n","        vec = vec / np.linalg.norm(vec)  # normalize\n","        sim = np.dot(target_vec, vec)\n","        similarities[word] = sim\n","\n","    # Get the top N words sorted by similarity (highest first)\n","    sorted_sim = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n","    return sorted_sim[:top_n]\n","\n","# Example usage:\n","# (Assume final_embeddings is already on CPU and in NumPy array format)\n","# For example, let's say we want to test the analogy:\n","# \"भारतीय\" is to \"अंतरिक्ष\" as \"SomeOtherWord\" is to ?\n","# Replace 'SomeOtherWord' with a word that is in your vocabulary.\n","\n","analogy_result = word_analogy(\"राजा\", \"रानी\", \"पुरुष\" ,\n","                               final_embeddings, word_to_id, id_to_word, top_n=5)\n","if analogy_result:\n","    print(\"Analogy results (word, similarity):\")\n","    for word, sim in analogy_result:\n","        print(f\"{word}: {sim:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CDJo0ubdq81w","executionInfo":{"status":"ok","timestamp":1741794124253,"user_tz":-330,"elapsed":389,"user":{"displayName":"MultiDL","userId":"05750408194492004447"}},"outputId":"99d692ff-764e-4238-e225-1e587a28fa77"},"execution_count":133,"outputs":[{"output_type":"stream","name":"stdout","text":["Analogy results (word, similarity):\n","हॉकी: 0.3599\n","महिला: 0.3387\n","वेस्टइंडीज: 0.3266\n","संबंध: 0.3212\n","आबेदीन: 0.3152\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyOb4+TW5i/sdBheJbKpACbm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}